{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ahatesham02/Bangla-Image-Captioning/blob/main/Bangla_Image_Captioning_with_attention_bancap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYnLFZpV3hgn"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import keras\n",
        "from numpy import array\n",
        "from pickle import load\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.backend import set_session\n",
        "import sys, time, os, warnings \n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "from PIL import Image\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from keras.utils import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.utils import plot_model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense, BatchNormalization\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import add\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.utils import load_img, img_to_array\n",
        "from sklearn.utils import shuffle\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "the6f56J37oA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51448eac-4fd7-4fdb-d1ee-1b8c7b1000d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "[Flickr8k_Dataset.zip]\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of Flickr8k_Dataset.zip or\n",
            "        Flickr8k_Dataset.zip.zip, and cannot find Flickr8k_Dataset.zip.ZIP, period.\n"
          ]
        }
      ],
      "source": [
        "!wget -q https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "\n",
        "!unzip -qq Flickr8k_Dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3_OB9d27fon",
        "outputId": "4915547f-0d66-4a08-f0a4-7e91733b14f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting bnlp_toolkit\n",
            "  Downloading bnlp_toolkit-3.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: wasabi in /usr/local/lib/python3.8/dist-packages (from bnlp_toolkit) (0.10.1)\n",
            "Collecting gensim==4.0.1\n",
            "  Downloading gensim-4.0.1-cp38-cp38-manylinux1_x86_64.whl (23.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.9 MB 10.7 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 55.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from bnlp_toolkit) (1.7.3)\n",
            "Collecting sklearn-crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from bnlp_toolkit) (4.64.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from bnlp_toolkit) (3.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from bnlp_toolkit) (1.21.6)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from gensim==4.0.1->bnlp_toolkit) (6.3.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->bnlp_toolkit) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->bnlp_toolkit) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->bnlp_toolkit) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sklearn-crfsuite->bnlp_toolkit) (1.15.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from sklearn-crfsuite->bnlp_toolkit) (0.8.10)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "  Downloading python_crfsuite-0.9.8-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 67.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite, sentencepiece, gensim, bnlp-toolkit\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "Successfully installed bnlp-toolkit-3.2.0 gensim-4.0.1 python-crfsuite-0.9.8 sentencepiece-0.1.97 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ],
      "source": [
        "! pip install bnlp_toolkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9l9fMquc7jyS",
        "outputId": "cbc447b0-88d8-4a60-8505-00caec23759a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "punkt not found. downloading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import bnlp\n",
        "from bnlp.corpus import  punctuations, letters, digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vciu-JZd5-G8"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Thesis/Flicker8k Bangla/BAN-Cap_captiondata.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "AaaBRdDK7NGb",
        "outputId": "cd59c532-c5d1-4c06-b8ce-7b580d6e0e47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                     caption_id  \\\n",
              "0   1000268201_693b08cb0e.jpg#0   \n",
              "1   1000268201_693b08cb0e.jpg#1   \n",
              "2   1000268201_693b08cb0e.jpg#2   \n",
              "3   1000268201_693b08cb0e.jpg#3   \n",
              "4   1000268201_693b08cb0e.jpg#4   \n",
              "5   1001773457_577c3a7d70.jpg#0   \n",
              "6   1001773457_577c3a7d70.jpg#1   \n",
              "7   1001773457_577c3a7d70.jpg#2   \n",
              "8   1001773457_577c3a7d70.jpg#3   \n",
              "9   1001773457_577c3a7d70.jpg#4   \n",
              "10  1002674143_1b742ab4b8.jpg#0   \n",
              "11  1002674143_1b742ab4b8.jpg#1   \n",
              "12  1002674143_1b742ab4b8.jpg#2   \n",
              "13  1002674143_1b742ab4b8.jpg#3   \n",
              "14  1002674143_1b742ab4b8.jpg#4   \n",
              "\n",
              "                                      english_caption  \\\n",
              "0   A child in a pink dress is climbing up a set o...   \n",
              "1               A girl going into a wooden building .   \n",
              "2    A little girl climbing into a wooden playhouse .   \n",
              "3   A little girl climbing the stairs to her playh...   \n",
              "4   A little girl in a pink dress going into a woo...   \n",
              "5          A black dog and a spotted dog are fighting   \n",
              "6   A black dog and a tri-colored dog playing with...   \n",
              "7   A black dog and a white dog with brown spots a...   \n",
              "8   Two dogs of different breeds looking at each o...   \n",
              "9     Two dogs on pavement moving toward each other .   \n",
              "10  A little girl covered in paint sits in front o...   \n",
              "11  A little girl is sitting in front of a large p...   \n",
              "12  A small girl in the grass plays with fingerpai...   \n",
              "13  There is a girl with pigtails sitting in front...   \n",
              "14  Young girl with pigtails painting outside in t...   \n",
              "\n",
              "                                      bengali_caption  \n",
              "0   একটি গোলাপী জামা পরা বাচ্চা মেয়ে একটি বাড়ির প্...  \n",
              "1              একটি মেয়ে শিশু একটি কাঠের বাড়িতে ঢুকছে  \n",
              "2                একটি বাচ্চা তার কাঠের খেলাঘরে উঠছে ।  \n",
              "3            ছোট মেয়েটি তার খেলার ঘরের সিড়ি বেয়ে উঠছে  \n",
              "4   গোলাপি জামা পড়া ছোট একটি মেয়ে একটি কাঠের তৈরি...  \n",
              "5   একটি কালো কুকুর এবং একটি ছোপওয়ালা কুকুর ঝগড়া করছে  \n",
              "6   একটি কালো কুকুর একটি তিন রঙা কুকুরের সাথে রাস্...  \n",
              "7   একটি কালো কুকুর ও একটি সাদা-বাদামি ছোপযুক্ত কু...  \n",
              "8   ভিন্ন জাতের দুটি কুকুর রাস্তায় একে অপরের দিকে...  \n",
              "9   রাস্তার পাশে দুইটি কুকুর পরস্পরের দিকে এগিয়ে য...  \n",
              "10  রঙে মাখা একটি ছোট মেয়ে হাতে একটি রঙের বাটিতে হ...  \n",
              "11  একটি ছোটো মেয়ে একটি হাতে আঁকা রংধনুর সামনে বস...  \n",
              "12  একটি ছোটো মেয়ে ঘাসে বসে আঙুলে রং নিয়ে খেলা করছ...  \n",
              "13  চুলে বেণী করা ছোট মেয়েটি একটি রংধনু চিত্রের সা...  \n",
              "14  চুলে বেনী করা অল্প বয়সী মেয়েটি বাইরে ঘাসের উপর...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-805da23e-edfd-4b43-b029-624fbcef533c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>caption_id</th>\n",
              "      <th>english_caption</th>\n",
              "      <th>bengali_caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000268201_693b08cb0e.jpg#0</td>\n",
              "      <td>A child in a pink dress is climbing up a set o...</td>\n",
              "      <td>একটি গোলাপী জামা পরা বাচ্চা মেয়ে একটি বাড়ির প্...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000268201_693b08cb0e.jpg#1</td>\n",
              "      <td>A girl going into a wooden building .</td>\n",
              "      <td>একটি মেয়ে শিশু একটি কাঠের বাড়িতে ঢুকছে</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000268201_693b08cb0e.jpg#2</td>\n",
              "      <td>A little girl climbing into a wooden playhouse .</td>\n",
              "      <td>একটি বাচ্চা তার কাঠের খেলাঘরে উঠছে ।</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000268201_693b08cb0e.jpg#3</td>\n",
              "      <td>A little girl climbing the stairs to her playh...</td>\n",
              "      <td>ছোট মেয়েটি তার খেলার ঘরের সিড়ি বেয়ে উঠছে</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000268201_693b08cb0e.jpg#4</td>\n",
              "      <td>A little girl in a pink dress going into a woo...</td>\n",
              "      <td>গোলাপি জামা পড়া ছোট একটি মেয়ে একটি কাঠের তৈরি...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1001773457_577c3a7d70.jpg#0</td>\n",
              "      <td>A black dog and a spotted dog are fighting</td>\n",
              "      <td>একটি কালো কুকুর এবং একটি ছোপওয়ালা কুকুর ঝগড়া করছে</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1001773457_577c3a7d70.jpg#1</td>\n",
              "      <td>A black dog and a tri-colored dog playing with...</td>\n",
              "      <td>একটি কালো কুকুর একটি তিন রঙা কুকুরের সাথে রাস্...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1001773457_577c3a7d70.jpg#2</td>\n",
              "      <td>A black dog and a white dog with brown spots a...</td>\n",
              "      <td>একটি কালো কুকুর ও একটি সাদা-বাদামি ছোপযুক্ত কু...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1001773457_577c3a7d70.jpg#3</td>\n",
              "      <td>Two dogs of different breeds looking at each o...</td>\n",
              "      <td>ভিন্ন জাতের দুটি কুকুর রাস্তায় একে অপরের দিকে...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1001773457_577c3a7d70.jpg#4</td>\n",
              "      <td>Two dogs on pavement moving toward each other .</td>\n",
              "      <td>রাস্তার পাশে দুইটি কুকুর পরস্পরের দিকে এগিয়ে য...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1002674143_1b742ab4b8.jpg#0</td>\n",
              "      <td>A little girl covered in paint sits in front o...</td>\n",
              "      <td>রঙে মাখা একটি ছোট মেয়ে হাতে একটি রঙের বাটিতে হ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1002674143_1b742ab4b8.jpg#1</td>\n",
              "      <td>A little girl is sitting in front of a large p...</td>\n",
              "      <td>একটি ছোটো মেয়ে একটি হাতে আঁকা রংধনুর সামনে বস...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1002674143_1b742ab4b8.jpg#2</td>\n",
              "      <td>A small girl in the grass plays with fingerpai...</td>\n",
              "      <td>একটি ছোটো মেয়ে ঘাসে বসে আঙুলে রং নিয়ে খেলা করছ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1002674143_1b742ab4b8.jpg#3</td>\n",
              "      <td>There is a girl with pigtails sitting in front...</td>\n",
              "      <td>চুলে বেণী করা ছোট মেয়েটি একটি রংধনু চিত্রের সা...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1002674143_1b742ab4b8.jpg#4</td>\n",
              "      <td>Young girl with pigtails painting outside in t...</td>\n",
              "      <td>চুলে বেনী করা অল্প বয়সী মেয়েটি বাইরে ঘাসের উপর...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-805da23e-edfd-4b43-b029-624fbcef533c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-805da23e-edfd-4b43-b029-624fbcef533c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-805da23e-edfd-4b43-b029-624fbcef533c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "data.head(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oj_dmOLi7pyd"
      },
      "outputs": [],
      "source": [
        "def create_vocabulary(data):\n",
        "  vocab = []\n",
        "  for captions in data.bengali_caption.values:\n",
        "    vocab.extend(captions.split())\n",
        "  print(\"Vocabulary Size : {}\".format(len(set(vocab))))\n",
        "  return vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9zQmYDM8P0m",
        "outputId": "66356024-5360-40f8-ff0e-a4ce2314cd4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size : 17351\n"
          ]
        }
      ],
      "source": [
        "vocabulary = create_vocabulary(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo6XnfQZ8YuD"
      },
      "outputs": [],
      "source": [
        "table = str.maketrans('', '', punctuations)\n",
        "desc = [w.translate(table) for w in data.bengali_caption]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D06mfWp88gw-"
      },
      "outputs": [],
      "source": [
        "def cleaning(row):\n",
        "  text = re.sub('[^\\u0980-\\u09FF]',' ', row) \n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xVb7yQg8swl"
      },
      "outputs": [],
      "source": [
        "data['bengali_caption'] = data.bengali_caption.apply(cleaning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "yqGm2QYM9ABf",
        "outputId": "a467341e-5944-4346-d541-06b45b87c515"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                    caption_id  \\\n",
              "0  1000268201_693b08cb0e.jpg#0   \n",
              "1  1000268201_693b08cb0e.jpg#1   \n",
              "2  1000268201_693b08cb0e.jpg#2   \n",
              "3  1000268201_693b08cb0e.jpg#3   \n",
              "4  1000268201_693b08cb0e.jpg#4   \n",
              "5  1001773457_577c3a7d70.jpg#0   \n",
              "6  1001773457_577c3a7d70.jpg#1   \n",
              "7  1001773457_577c3a7d70.jpg#2   \n",
              "8  1001773457_577c3a7d70.jpg#3   \n",
              "9  1001773457_577c3a7d70.jpg#4   \n",
              "\n",
              "                                     english_caption  \\\n",
              "0  A child in a pink dress is climbing up a set o...   \n",
              "1              A girl going into a wooden building .   \n",
              "2   A little girl climbing into a wooden playhouse .   \n",
              "3  A little girl climbing the stairs to her playh...   \n",
              "4  A little girl in a pink dress going into a woo...   \n",
              "5         A black dog and a spotted dog are fighting   \n",
              "6  A black dog and a tri-colored dog playing with...   \n",
              "7  A black dog and a white dog with brown spots a...   \n",
              "8  Two dogs of different breeds looking at each o...   \n",
              "9    Two dogs on pavement moving toward each other .   \n",
              "\n",
              "                                     bengali_caption  \n",
              "0  একটি গোলাপী জামা পরা বাচ্চা মেয়ে একটি বাড়ির প্...  \n",
              "1             একটি মেয়ে শিশু একটি কাঠের বাড়িতে ঢুকছে  \n",
              "2               একটি বাচ্চা তার কাঠের খেলাঘরে উঠছে    \n",
              "3           ছোট মেয়েটি তার খেলার ঘরের সিড়ি বেয়ে উঠছে  \n",
              "4  গোলাপি জামা পড়া ছোট একটি মেয়ে একটি কাঠের তৈরি...  \n",
              "5  একটি কালো কুকুর এবং একটি ছোপওয়ালা কুকুর ঝগড়া করছে  \n",
              "6  একটি কালো কুকুর একটি তিন রঙা কুকুরের সাথে রাস্...  \n",
              "7  একটি কালো কুকুর ও একটি সাদা বাদামি ছোপযুক্ত কু...  \n",
              "8  ভিন্ন জাতের দুটি কুকুর রাস্তায় একে অপরের দিকে...  \n",
              "9  রাস্তার পাশে দুইটি কুকুর পরস্পরের দিকে এগিয়ে য...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4a6012c2-267d-4f1c-80cf-adf7527b14f9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>caption_id</th>\n",
              "      <th>english_caption</th>\n",
              "      <th>bengali_caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000268201_693b08cb0e.jpg#0</td>\n",
              "      <td>A child in a pink dress is climbing up a set o...</td>\n",
              "      <td>একটি গোলাপী জামা পরা বাচ্চা মেয়ে একটি বাড়ির প্...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000268201_693b08cb0e.jpg#1</td>\n",
              "      <td>A girl going into a wooden building .</td>\n",
              "      <td>একটি মেয়ে শিশু একটি কাঠের বাড়িতে ঢুকছে</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000268201_693b08cb0e.jpg#2</td>\n",
              "      <td>A little girl climbing into a wooden playhouse .</td>\n",
              "      <td>একটি বাচ্চা তার কাঠের খেলাঘরে উঠছে</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000268201_693b08cb0e.jpg#3</td>\n",
              "      <td>A little girl climbing the stairs to her playh...</td>\n",
              "      <td>ছোট মেয়েটি তার খেলার ঘরের সিড়ি বেয়ে উঠছে</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000268201_693b08cb0e.jpg#4</td>\n",
              "      <td>A little girl in a pink dress going into a woo...</td>\n",
              "      <td>গোলাপি জামা পড়া ছোট একটি মেয়ে একটি কাঠের তৈরি...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1001773457_577c3a7d70.jpg#0</td>\n",
              "      <td>A black dog and a spotted dog are fighting</td>\n",
              "      <td>একটি কালো কুকুর এবং একটি ছোপওয়ালা কুকুর ঝগড়া করছে</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1001773457_577c3a7d70.jpg#1</td>\n",
              "      <td>A black dog and a tri-colored dog playing with...</td>\n",
              "      <td>একটি কালো কুকুর একটি তিন রঙা কুকুরের সাথে রাস্...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1001773457_577c3a7d70.jpg#2</td>\n",
              "      <td>A black dog and a white dog with brown spots a...</td>\n",
              "      <td>একটি কালো কুকুর ও একটি সাদা বাদামি ছোপযুক্ত কু...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1001773457_577c3a7d70.jpg#3</td>\n",
              "      <td>Two dogs of different breeds looking at each o...</td>\n",
              "      <td>ভিন্ন জাতের দুটি কুকুর রাস্তায় একে অপরের দিকে...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1001773457_577c3a7d70.jpg#4</td>\n",
              "      <td>Two dogs on pavement moving toward each other .</td>\n",
              "      <td>রাস্তার পাশে দুইটি কুকুর পরস্পরের দিকে এগিয়ে য...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4a6012c2-267d-4f1c-80cf-adf7527b14f9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4a6012c2-267d-4f1c-80cf-adf7527b14f9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4a6012c2-267d-4f1c-80cf-adf7527b14f9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hcbwgjp9Cnd",
        "outputId": "aa24c40f-5860-41f5-da9c-8bd149f6a512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size : 15660\n"
          ]
        }
      ],
      "source": [
        "clean_vocabulary = create_vocabulary(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8nz3KNa9QKR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "outputId": "db601abd-d6dd-4e57-8ad2-43c6646768c9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-5770ff54fb73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m## The location of the Flickr8K_ photos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimage_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/Flicker8k_Dataset'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/Flicker8k_Dataset'"
          ]
        }
      ],
      "source": [
        "from os import listdir\n",
        "## The location of the Flickr8K_ photos\n",
        "image_dir = '/content/Flicker8k_Dataset'\n",
        "images = listdir(image_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6damaXlV95aO"
      },
      "outputs": [],
      "source": [
        "def clean(col):\n",
        "  text = col.split('#')[0]\n",
        "  return text\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1xMh6AbAbsM"
      },
      "outputs": [],
      "source": [
        "data2 = data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-OiVOwNAffb"
      },
      "outputs": [],
      "source": [
        "data2.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7vcFjoPPYNo"
      },
      "outputs": [],
      "source": [
        "data2['caption_id'] = data2.caption_id.apply(lambda x : x.split('#')[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWU1q6kL_w-7"
      },
      "outputs": [],
      "source": [
        "data2.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xzit6L60-kqu"
      },
      "outputs": [],
      "source": [
        "data[7000:7010]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gf7TicA3_2C2"
      },
      "outputs": [],
      "source": [
        "data2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZeZQddxBSsC"
      },
      "outputs": [],
      "source": [
        "data3 = data2[:40000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UV46Jz7TBXKW"
      },
      "outputs": [],
      "source": [
        "data3.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNDW6dm4_7SF"
      },
      "outputs": [],
      "source": [
        "def preprocess_images(data):\n",
        "  img_name_vector = []\n",
        "\n",
        "  for filenames in data3[\"caption_id\"]:\n",
        "      full_image_path = image_dir+\"/\"+ filenames\n",
        "      img_name_vector.append(full_image_path)\n",
        "  return img_name_vector\n",
        "img_name_vector = preprocess_images(data)\n",
        "img_name_vector[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2_O7lOA99n3"
      },
      "outputs": [],
      "source": [
        "def preprocess_captions(data):\n",
        "  total_captions = []\n",
        "\n",
        "  for caption  in data3[\"bengali_caption\"].astype(str):\n",
        "      caption = '<start> ' + caption+ ' <end>'\n",
        "      total_captions.append(caption)\n",
        "  return total_captions\n",
        "total_captions = preprocess_captions(data)\n",
        "total_captions[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lFxR4cYCiLG"
      },
      "outputs": [],
      "source": [
        "\"\"\"def data_limiter(num,total_captions,img_name_vector):\n",
        "  # Shuffle captions and image_names together\n",
        "  train_captions, img_name_vector = shuffle(total_captions,img_name_vector,random_state=1)\n",
        "  train_captions = train_captions[:num]\n",
        "  img_name_vector = img_name_vector[:num]\n",
        "  return train_captions,img_name_vector\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1sPOCDkHCukL"
      },
      "outputs": [],
      "source": [
        "#total_captions,img_name_vector = data_limiter(40000,total_captions,img_name_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah5vr4fXC4Cy"
      },
      "outputs": [],
      "source": [
        "len(total_captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Tx8wnPiC696"
      },
      "outputs": [],
      "source": [
        "from pickle import dump\n",
        "\n",
        "# save to file\n",
        "dump(total_captions, open('bengali_captions.pkl', 'wb'))\n",
        "dump(img_name_vector, open('bengali_img_names.pkl', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6L97uAuDDn0"
      },
      "outputs": [],
      "source": [
        "# To know the shape of images\n",
        "def img_shape_finder(image):\n",
        "  img= plt.imread(image)\n",
        "\n",
        "  print(\"Shape of the image ==> {0} is ==> {1}\".format(image.split('/')[1],img.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4yAwOJI8D1WR"
      },
      "outputs": [],
      "source": [
        "img_list=[]\n",
        "for i in range(20):\n",
        "  img_list.append(img_name_vector[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOKgeX3TD7UI"
      },
      "outputs": [],
      "source": [
        "for j in img_list:\n",
        "  img_shape_finder(j)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWNkna97D9SI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YUOv3m9EBhO"
      },
      "outputs": [],
      "source": [
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEZ02RmjCjr6"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdmaki75CjhP"
      },
      "outputs": [],
      "source": [
        "image_model2 = ResNet50(include_top=False,weights='imagenet')\n",
        "new_input = image_model2.input \n",
        "hidden_layer = image_model2.layers[-1].output  \n",
        "image_features_extract_model2 = tf.compat.v1.keras.Model(new_input, hidden_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Njr8NWWFCjOG"
      },
      "outputs": [],
      "source": [
        "image_features_extract_model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNWSSvjpEEQp"
      },
      "outputs": [],
      "source": [
        "preprocessed_image = []\n",
        "IMAGE_SHAPE = (224, 224)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dro1rSorESXx"
      },
      "outputs": [],
      "source": [
        "for img in img_name_vector[0:5] :\n",
        "    img = tf.io.read_file(img, name=None)\n",
        "    img = tf.image.decode_jpeg(img, channels=0)\n",
        "    img = tf.image.resize(img, (224, 224))\n",
        "    img = tf.keras.applications.resnet50.preprocess_input(img)\n",
        "    preprocessed_image.append(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtZYGRpdEUhV"
      },
      "outputs": [],
      "source": [
        "# checking first five images post preprocessing\n",
        "Display_Images = preprocessed_image[0:5]\n",
        "figure, axes = plt.subplots(1,5)\n",
        "figure.set_figwidth(25)\n",
        "for ax, image in zip(axes, Display_Images) :\n",
        "  print('Shape after resize : ', image.shape)\n",
        "  ax.imshow(image)\n",
        "  ax.grid('off')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-PHMRVrEYJR"
      },
      "outputs": [],
      "source": [
        "def load_images(image_path) :\n",
        "  img = tf.io.read_file(image_path, name = None)\n",
        "  img = tf.image.decode_jpeg(img, channels=0)\n",
        "  img = tf.image.resize(img, IMAGE_SHAPE)\n",
        "  img = tf.keras.applications.resnet50.preprocess_input(img)\n",
        "  return img, image_path\n",
        "\n",
        "img1,img1_path = load_images(\"Flicker8k_Dataset/3439243433_d5f3508612.jpg\")\n",
        "print(\"Shape after resize :\", img1.shape)\n",
        "plt.imshow(img1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5Vy7-W2EciY"
      },
      "outputs": [],
      "source": [
        "image_model = tf.keras.applications.InceptionV3(include_top=False,weights='imagenet')\n",
        "new_input = image_model.input \n",
        "hidden_layer = image_model.layers[-1].output  \n",
        "image_features_extract_model = tf.compat.v1.keras.Model(new_input, hidden_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yO_J24aREgOB"
      },
      "outputs": [],
      "source": [
        "image_features_extract_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YvrCy76rqs1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz-RoKi8rlAC"
      },
      "outputs": [],
      "source": [
        "image_model2 = ResNet50(include_top=False,weights='imagenet')\n",
        "new_input = image_model2.input \n",
        "hidden_layer = image_model2.layers[-1].output  \n",
        "image_features_extract_model2 = tf.compat.v1.keras.Model(new_input, hidden_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6dVhDVMrxSu"
      },
      "outputs": [],
      "source": [
        "image_features_extract_model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWQSzifMEi9V"
      },
      "outputs": [],
      "source": [
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "                                                 oov_token=\"<unk>\",\n",
        "                                                 filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "\n",
        "tokenizer.fit_on_texts(total_captions)\n",
        "train_seqs = tokenizer.texts_to_sequences(total_captions)\n",
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'\n",
        "\n",
        "train_seqs = tokenizer.texts_to_sequences(total_captions)\n",
        "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1caW86y8ExWA"
      },
      "outputs": [],
      "source": [
        "vocab_size = len(tokenizer.index_word)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p68EkHWuE4GK"
      },
      "outputs": [],
      "source": [
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZDs3SdyE0iv"
      },
      "outputs": [],
      "source": [
        "total_captions[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3j7fFpiE7nv"
      },
      "outputs": [],
      "source": [
        "train_seqs[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_vW6vFzFA48"
      },
      "outputs": [],
      "source": [
        "tokenizer.index_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6d7drnrHFFOu"
      },
      "outputs": [],
      "source": [
        "# Pad each vector to the max_length of the captions  store it to a vairable\n",
        "\n",
        "train_seqs_len = [len(seq) for seq in train_seqs]\n",
        "\n",
        "longest_word_length = max(train_seqs_len)\n",
        "\n",
        "cap_vector= tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding= 'post', maxlen = longest_word_length,\n",
        "                                                          dtype='int32', value=0)\n",
        "print(\"The shape of Caption vector is :\" + str(cap_vector.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXnwQTleFM8y"
      },
      "outputs": [],
      "source": [
        "cap_vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvGWs7DzFS9G"
      },
      "outputs": [],
      "source": [
        "# Map each image full path to the function, in order to preprocess the image\n",
        "training_list = sorted(set(img_name_vector))\n",
        "New_Img = tf.data.Dataset.from_tensor_slices(training_list)\n",
        "New_Img = New_Img.map(load_images, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "New_Img = New_Img.batch(64, drop_remainder=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTsIOppzFY2T"
      },
      "outputs": [],
      "source": [
        "path_train, path_test, caption_train, caption_test = train_test_split(img_name_vector, cap_vector, test_size = 0.1, random_state = 42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tn55K4PcFdWT"
      },
      "outputs": [],
      "source": [
        "print(\"Training data for images: \" + str(len(path_train)))\n",
        "print(\"Testing data for images: \" + str(len(path_test)))\n",
        "print(\"Training data for Captions: \" + str(len(caption_train)))\n",
        "print(\"Testing data for Captions: \" + str(len(caption_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEV-jYUWFfrL"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ7f-Ws7FkBA"
      },
      "outputs": [],
      "source": [
        "# extract features from each image in the dataset\n",
        "img_features = {}\n",
        "for image, image_path in tqdm(New_Img) :\n",
        "  batch_features = image_features_extract_model2(image)\n",
        "  #squeeze out the features in a batch\n",
        "  batch_features_flattened = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "  for batch_feat, path in zip(batch_features_flattened, image_path) :\n",
        "    feature_path = path.numpy().decode('utf-8')\n",
        "    img_features[feature_path] = batch_feat.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYuDOCYEFmY7"
      },
      "outputs": [],
      "source": [
        "batch_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCx-1k9rFtSJ"
      },
      "outputs": [],
      "source": [
        "img_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04aaeB9zFyqo"
      },
      "outputs": [],
      "source": [
        "#to provide, both images along with the captions as input\n",
        "def map(image_name, caption):\n",
        "    img_tensor = img_features[image_name.decode('utf-8')]\n",
        "    return img_tensor, caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gt35Uaf6F4nE"
      },
      "outputs": [],
      "source": [
        "# This function should transform the created dataset(img_path,cap) to (features,cap) using the map_func created earlier\n",
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 64\n",
        "def gen_dataset(img, capt):\n",
        "        \n",
        "    data = tf.data.Dataset.from_tensor_slices((img, capt))\n",
        "    data = data.map(lambda ele1, ele2 : tf.numpy_function(map, [ele1, ele2], [tf.float32, tf.int32]),\n",
        "                    num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "    \n",
        "     \n",
        "    data = (data.shuffle(BUFFER_SIZE, reshuffle_each_iteration= True).batch(BATCH_SIZE, drop_remainder = False)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ADybd0gPF70t"
      },
      "outputs": [],
      "source": [
        "train_dataset = gen_dataset(path_train,caption_train)\n",
        "test_dataset = gen_dataset(path_test,caption_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FchDw6CzF93m"
      },
      "outputs": [],
      "source": [
        "sample_img_batch, sample_cap_batch = next(iter(train_dataset))\n",
        "print(sample_img_batch.shape)  #(batch_size, 8*8, 2048)\n",
        "print(sample_cap_batch.shape) #(batch_size,max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThRBHOKpGABw"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = vocab_size\n",
        "train_num_steps = len(path_train) // BATCH_SIZE \n",
        "test_num_steps = len(path_test) // BATCH_SIZE \n",
        "EPOCHS = 20\n",
        "max_length = 34\n",
        "feature_shape = batch_feat.shape[1]\n",
        "attention_feature_shape = batch_feat.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPCq6JhcGEKp"
      },
      "outputs": [],
      "source": [
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "print(tf.compat.v1.get_default_graph())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpgicVsNGMa5"
      },
      "outputs": [],
      "source": [
        "#Building Encoder using CNN Keras subclassing method\n",
        "\n",
        "class Encoder(Model):\n",
        "    def __init__(self,embed_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dense = tf.keras.layers.Dense(embed_dim) #build your Dense layer with relu activation\n",
        "        \n",
        "    def call(self, features):\n",
        "        features =  self.dense(features) # extract the features from the image shape: (batch, 8*8, embed_dim)\n",
        "        features =  tf.keras.activations.relu(features, alpha=0.01, max_value=None, threshold=0)\n",
        "        return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JF9H7XQ_GQD2"
      },
      "outputs": [],
      "source": [
        "encoder=Encoder(embedding_dim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2LF8MuHGSfJ"
      },
      "outputs": [],
      "source": [
        "class Bahdanau_Attention(Model):\n",
        "    def __init__(self, units):\n",
        "        super(Bahdanau_Attention, self).__init__()\n",
        "        self.units=units\n",
        "        self.W1 = tf.keras.layers.Dense(units) \n",
        "        self.W2 = tf.keras.layers.Dense(units) \n",
        "        self.V = tf.keras.layers.Dense(1) \n",
        "        \n",
        "\n",
        "    def call(self, features, hidden):\n",
        "        hidden_with_time_axis = hidden[:, tf.newaxis]\n",
        "        score = tf.keras.activations.tanh(self.W1(features) + self.W2(hidden_with_time_axis))  \n",
        "        attention_weights = tf.keras.activations.softmax(self.V(score), axis=1) \n",
        "        context_vector = attention_weights * features \n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)  \n",
        "        return context_vector, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbirdPcSGWm_"
      },
      "outputs": [],
      "source": [
        "class Decoder(Model):\n",
        "    def __init__(self, embed_dim, units, vocab_size):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.units=units\n",
        "        self.attention = Bahdanau_Attention(self.units) #iniitalise your Attention model with units\n",
        "        self.embed = tf.keras.layers.Embedding(vocab_size, embed_dim) #build your Embedding layer\n",
        "        self.gru = tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
        "        self.d1 = tf.keras.layers.Dense(self.units) #build your Dense layer\n",
        "        self.d2 = tf.keras.layers.Dense(vocab_size) #build your Dense layer\n",
        "        \n",
        "\n",
        "    def call(self,x,features, hidden):\n",
        "        context_vector, attention_weights = self.attention(features, hidden) #create your context vector & attention weights from attention model\n",
        "        embed = self.embed(x) # embed your input to shape: (batch_size, 1, embedding_dim)\n",
        "        embed = tf.concat([tf.expand_dims(context_vector, 1), embed], axis = -1) # Concatenate your input with the context vector from attention layer. Shape: (batch_size, 1, embedding_dim + embedding_dim)\n",
        "        output,state = self.gru(embed) # Extract the output & hidden state from GRU layer. Output shape : (batch_size, max_length, hidden_size)\n",
        "        output = self.d1(output)\n",
        "        output = tf.reshape(output, (-1, output.shape[2])) # shape : (batch_size * max_length, hidden_size)\n",
        "        output = self.d2(output) # shape : (batch_size * max_length, vocab_size)\n",
        "        return output, state, attention_weights\n",
        "    \n",
        "    def init_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqSzvB79BAsl"
      },
      "outputs": [],
      "source": [
        "class Decoder2(Model):\n",
        "    def __init__(self, embed_dim, units, vocab_size):\n",
        "        super(Decoder2, self).__init__()\n",
        "        self.units=units\n",
        "        self.attention = Bahdanau_Attention(self.units) #iniitalise your Attention model with units\n",
        "        self.embed = tf.keras.layers.Embedding(vocab_size, embed_dim) #build your Embedding layer\n",
        "        self.gru = tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
        "        self.d1 = tf.keras.layers.Dense(self.units) #build your Dense layer\n",
        "        self.d2 = tf.keras.layers.Dense(vocab_size) #build your Dense layer\n",
        "        self.dense = tf.keras.layers.Dense(embed_dim)\n",
        "        \n",
        "\n",
        "    def call(self,x,features, hidden):\n",
        "        context_vector1, attention_weights1 = self.attention(features, hidden) #create your context vector & attention weights from attention model\n",
        "        embed = self.embed(x) # embed your input to shape: (batch_size, 1, embedding_dim)\n",
        "        embed1 = tf.concat([tf.expand_dims(context_vector1, 1), embed], axis = -1) # Concatenate your input with the context vector from attention layer. Shape: (batch_size, 1, embedding_dim + embedding_dim)\n",
        "        output1,state1= self.gru(embed1) # Extract the output & hidden state from GRU layer. Output shape : (batch_size, max_length, hidden_size)\n",
        "        output1 = self.dense(output1)\n",
        "        #state1 = self.dense(state1)\n",
        "        context_vector2, attention_weights2 = self.attention(output1, state1)\n",
        "        context_vector = tf.keras.layers.Add()([context_vector1, context_vector2])\n",
        "        embed2 = tf.concat([tf.expand_dims(context_vector, 1), embed], axis = -1)\n",
        "        output,state= self.gru(embed2)\n",
        "        attention_weights = tf.keras.layers.Add()([attention_weights1, attention_weights2])\n",
        "        output = self.d1(output)\n",
        "        output = tf.reshape(output, (-1, output.shape[2])) # shape : (batch_size * max_length, hidden_size)\n",
        "        output = self.d2(output) # shape : (batch_size * max_length, vocab_size)\n",
        "        return output, state, attention_weights\n",
        "    \n",
        "    def init_state(self, batch_size):\n",
        "        return tf.zeros((batch_size, self.units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yx5sGKIfGaPy"
      },
      "outputs": [],
      "source": [
        "decoder=Decoder(embedding_dim, units, vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEXsnZBIGd64"
      },
      "outputs": [],
      "source": [
        "features=encoder(sample_img_batch)\n",
        "\n",
        "hidden = decoder.init_state(batch_size=sample_cap_batch.shape[0])\n",
        "dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * sample_cap_batch.shape[0], 1)\n",
        "\n",
        "predictions, hidden_out, attention_weights= decoder(dec_input, features, hidden)\n",
        "print('Feature shape from Encoder: {}'.format(features.shape)) #(batch, 8*8, embed_dim)\n",
        "print('Predcitions shape from Decoder: {}'.format(predictions.shape)) #(batch,vocab_size)\n",
        "print('Attention weights shape from Decoder: {}'.format(attention_weights.shape)) #(batch, 8*8, embed_dim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCgYTPxMGhEz"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)  #define the optimizer\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = tf.keras.losses.Reduction.NONE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJsqK-ezGl0y"
      },
      "outputs": [],
      "source": [
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "    #loss is getting multiplied with mask to get an ideal shape\n",
        "    \n",
        "    return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLyddmL3GqN2"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"/content/model_exp/\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer = optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw1ntFjQGuYF"
      },
      "outputs": [],
      "source": [
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJORSdvmGws3"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "    loss = 0\n",
        "    hidden = decoder.init_state(batch_size=target.shape[0])\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "        encoder_op = encoder(img_tensor)\n",
        "        for r in range(1, target.shape[1]) :\n",
        "          predictions, hidden, _ = decoder(dec_input, encoder_op, hidden)\n",
        "          loss = loss + loss_function(target[:, r], predictions) \n",
        "          dec_input = tf.expand_dims(target[:, r], 1)  \n",
        "\n",
        "    avg_loss = (loss/ int(target.shape[1])) #avg loss per batch\n",
        "    trainable_vars = encoder.trainable_variables + decoder.trainable_variables\n",
        "    grad = tape.gradient (loss, trainable_vars) \n",
        "    optimizer.apply_gradients(zip(grad, trainable_vars))\n",
        "\n",
        "    return loss, avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kwa-cMNeG1Kv"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def test_step(img_tensor, target):\n",
        "    loss = 0\n",
        "    hidden = decoder.init_state(batch_size = target.shape[0])\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "    with tf.GradientTape() as tape:\n",
        "      encoder_op = encoder(img_tensor)\n",
        "      for r in range(1, target.shape[1]) :\n",
        "        predictions, hidden, _ = decoder(dec_input, encoder_op, hidden)\n",
        "        loss = loss + loss_function(target[:, r], predictions)\n",
        "        dec_input = tf.expand_dims(target[: , r], 1)\n",
        "    avg_loss = (loss/ int(target.shape[1])) #avg loss per batch\n",
        "    trainable_vars = encoder.trainable_variables + decoder.trainable_variables\n",
        "    grad = tape.gradient (loss, trainable_vars) \n",
        "    optimizer.apply_gradients(zip(grad, trainable_vars))                      \n",
        "    return loss, avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ucyx7O-G4Ro"
      },
      "outputs": [],
      "source": [
        "def test_loss_cal(test_dataset):\n",
        "    total_loss = 0\n",
        "    for (batch, (img_tensor, target)) in enumerate(test_dataset) :\n",
        "      batch_loss, t_loss = test_step(img_tensor, target)\n",
        "      total_loss = total_loss + t_loss\n",
        "      avg_test_loss = total_loss/ test_num_steps\n",
        "\n",
        "    return avg_test_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5POwn0lG60w"
      },
      "outputs": [],
      "source": [
        "loss_plot = []\n",
        "test_loss_plot = []\n",
        "EPOCHS = 30\n",
        "best_test_loss=100\n",
        "for epoch in tqdm(range(0, EPOCHS)):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "        avg_train_loss=total_loss / train_num_steps   \n",
        "    loss_plot.append(avg_train_loss)    \n",
        "    test_loss = test_loss_cal(test_dataset)\n",
        "    test_loss_plot.append(test_loss)\n",
        "    print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
        "    if test_loss < best_test_loss:\n",
        "        print('Test loss has been reduced from %.3f to %.3f' % (best_test_loss, test_loss))\n",
        "        best_test_loss = test_loss\n",
        "        ckpt_manager.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aublRGuWG-ms"
      },
      "outputs": [],
      "source": [
        "ckpt_manager.checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EBl_WCHBMUlp"
      },
      "outputs": [],
      "source": [
        "ckpt.restore('/content/model_exp/ckpt-26')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXZe9UqXMaYA"
      },
      "outputs": [],
      "source": [
        "from matplotlib.pyplot import figure\n",
        "figure(figsize=(12, 8))\n",
        "plt.plot(loss_plot, color='orange', label = 'training_loss_plot')\n",
        "plt.plot(test_loss_plot, color='green', label = 'test_loss_plot')\n",
        "plt.xlabel('Epochs', fontsize = 15, color = 'red')\n",
        "plt.ylabel('Loss', fontsize = 15, color = 'red')\n",
        "plt.title('Loss Plot', fontsize = 20, color = 'red')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdujyLdVMkTG"
      },
      "outputs": [],
      "source": [
        "def evaluate(image):\n",
        "    attention_plot = np.zeros((max_length, attention_feature_shape))\n",
        "\n",
        "    hidden = decoder.init_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_images(image)[0], 0) \n",
        "    img_tensor_val = image_features_extract_model2(temp_input) \n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "\n",
        "    features = encoder (img_tensor_val) \n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden) \n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy() \n",
        "        result.append (tokenizer.index_word[predicted_id])\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, attention_plot,predictions\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot,predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iNbiQBXRMwtQ"
      },
      "outputs": [],
      "source": [
        "def plot_attention_map(caption, weights, image) :\n",
        "\n",
        "  fig = plt.figure(figsize = (30, 30))\n",
        "  temp_img = np.array(Image.open(image))\n",
        "\n",
        "  cap_len = len(caption)\n",
        "  for cap in range(len(caption)) :\n",
        "    weights_img = np.reshape(weights[cap], (7,7))\n",
        "    \n",
        "\n",
        "    ax = fig.add_subplot(cap_len//2, cap_len//2, cap+1)\n",
        "    ax.set_title(caption[cap])\n",
        "\n",
        "    img = ax.imshow(temp_img)\n",
        "\n",
        "    ax.imshow(weights_img, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
        "    ax.axis('off')\n",
        "  plt.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87W3NGRvM08x"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_s1ljk0uM4fk"
      },
      "outputs": [],
      "source": [
        "image_test = path_test.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBeXp_aQM6-I"
      },
      "outputs": [],
      "source": [
        "def filt_text(text):\n",
        "    filt=['<start>','<unk>','<end>'] \n",
        "    temp= text.split()\n",
        "    [temp.remove(j) for k in filt for j in temp if k==j]\n",
        "    text=' '.join(temp)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDVeAXRWM86o"
      },
      "outputs": [],
      "source": [
        "def pred_caption_audio(random, autoplay=False, weights=(0.5, 0.5, 0, 0)) :\n",
        "    cap_test_data = caption_test.copy()\n",
        "    rid = np.random.randint(0, len(path_test))\n",
        "    test_image= path_test[rid]\n",
        "    real_caption = ' '.join([tokenizer.index_word[i] for i in cap_test_data[rid] if i not in [0]])\n",
        "    result, attention_plot, pred_test = evaluate(test_image)\n",
        "    real_caption=filt_text(real_caption)      \n",
        "    pred_caption=' '.join(result).rsplit(' ', 1)[0]\n",
        "    real_appn = []\n",
        "    real_appn.append(real_caption.split())\n",
        "    reference = real_appn\n",
        "    candidate = pred_caption.split()\n",
        "    score = sentence_bleu(reference, candidate, weights=weights)#set your weights\n",
        "    print(f\"BLEU score: {score*100}\")\n",
        "    print ('Real Caption:', real_caption)\n",
        "    print ('Prediction Caption:', pred_caption)\n",
        "    plot_attention_map(result, attention_plot, test_image)\n",
        "    #speech = gTTS('Predicted Caption : ' + pred_caption, lang = 'bn', slow = False)\n",
        "    #speech.save('voice.mp3')\n",
        "    #audio_file = 'voice.mp3'\n",
        "    #display.display(display.Audio(audio_file, rate = None, autoplay = autoplay))\n",
        "\n",
        "    return test_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46m4Xu2kNBlQ"
      },
      "outputs": [],
      "source": [
        "test_image = pred_caption_audio(len(path_test), True, weights = (0.5, 0.5, 0, 0))\n",
        "Image.open(test_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT_51qzzNFnd"
      },
      "outputs": [],
      "source": [
        "cap_test_data = caption_test.copy()\n",
        "rid = np.random.randint(0, len(path_test))\n",
        "test_image= path_test[rid]\n",
        "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_test_data[rid] if i not in [0]])\n",
        "result, attention_plot, pred_test = evaluate(test_image)\n",
        "real_caption=filt_text(real_caption)      \n",
        "pred_caption=' '.join(result).rsplit(' ', 1)[0]\n",
        "real_appn = []\n",
        "real_appn.append(real_caption.split())\n",
        "reference = real_appn\n",
        "candidate = pred_caption.split()\n",
        "smooth = SmoothingFunction().method4\n",
        "bleu1 = sentence_bleu(reference, candidate, weights=(1.0, 0, 0, 0), smoothing_function=smooth)*100\n",
        "bleu2 = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth)*100\n",
        "bleu3 = sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.37, 0), smoothing_function=smooth)*100\n",
        "bleu4 = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)*100\n",
        "print('BLEU-1: %f' % bleu1)\n",
        "print('BLEU-2: %f' % bleu2)\n",
        "print('BLEU-3: %f' % bleu3)\n",
        "print('BLEU-4: %f' % bleu4)\n",
        "print ('Real Caption:', real_caption)\n",
        "print ('Prediction Caption:', pred_caption)\n",
        "plot_attention_map(result, attention_plot, test_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oR1KhMbiCKX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fX9Za3hvaWp6"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7dJllCPaaey"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.meteor_score import meteor_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zcDLOaTuHxfJ"
      },
      "outputs": [],
      "source": [
        "b1=[]\n",
        "b2=[]\n",
        "b3=[]\n",
        "b4=[]\n",
        "m_score = []\n",
        "\n",
        "for rid in range(0,len(path_test)):\n",
        "  test_image= path_test[rid]\n",
        "  real_caption = ' '.join([tokenizer.index_word[i] for i in caption_test[rid] if i not in [0]])\n",
        "  result, attention_plot, pred_test = evaluate(test_image)\n",
        "  real_caption=filt_text(real_caption)      \n",
        "  pred_caption=' '.join(result).rsplit(' ', 1)[0]\n",
        "  real_appn = []\n",
        "  real_appn.append(real_caption.split())\n",
        "  reference = real_appn\n",
        "  candidate = pred_caption.split()\n",
        "  smooth = SmoothingFunction().method4\n",
        "  bleu1 = sentence_bleu(reference, candidate, weights=(1.0, 0, 0, 0), smoothing_function=smooth)*100\n",
        "  bleu2 = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth)*100\n",
        "  bleu3 = sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.37, 0), smoothing_function=smooth)*100\n",
        "  bleu4 = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)*100\n",
        "  b1.append(bleu1)\n",
        "  b2.append(bleu2)\n",
        "  b3.append(bleu3)\n",
        "  b4.append(bleu4)\n",
        "  score = meteor_score(reference,candidate)\n",
        "  m_score.append(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ul2TG8VNH21c"
      },
      "outputs": [],
      "source": [
        "print('BLEU-1: %f' % np.mean(b1))\n",
        "print('BLEU-2: %f' % np.mean(b2))\n",
        "print('BLEU-3: %f' % np.mean(b3))\n",
        "print('BLEU-4: %f' % np.mean(b4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGTwazIKaG6z"
      },
      "outputs": [],
      "source": [
        "print('Meteor Score of Bahdanau attention: %f' % np.mean(m_score))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6U8QHKtahPQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1x_d4bMU1WVrVmb0WD81GDYLmXBwhGGNx",
      "authorship_tag": "ABX9TyOSJ7pYbSRR4xhUq3/Ssfe2",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}